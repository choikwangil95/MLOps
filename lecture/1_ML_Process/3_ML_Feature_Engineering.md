# 3 Feature Engineering

## 1 Data Cleansing

노이즈 데이터, 이상치 데이터, 누락된 데이터를 처리해서 모델이 데이터 분포를 잘 학습할 수 있도록 전처리
과정

- Noise Data 처리
  - 데이터 분포를 시각화하고 통계적 방법 또는 기계 학습 방법을 사용하여 식별하고 처리
  - 예를 들어, box plots 및 이상치 탐지 알고리즘을 활용할 수 있음
- Anomaly Data 처리
  - 이상치를 제거하거나 대체하는 두 가지 방법
  - 각 기 장단점이 있으며, 제거하는 경우 잘못된 데이터 제거 문제가 있을 수 있으며 대체는 데이터 분포를 왜곡할 수 잇다
- 3 누락 Data 처리
  - 누락된 데이터를 제거하거나 대체하는 두 가지 방법
  - 제거는 데이터 완전성을 유지하나, 데이터 수가 줄어들거나 class imbalance가 심해질 수 있음
  - 대체는 평균, 중간값, 최빈값 등의 대체 방법을 사용할 수 있으며, 머신러닝 모델을 활용한 예측도 있음. 하지만, 잘못된 대체로 데이터
    분포를 왜곡할 수 있음

## 2 Feature Selection

데이터의 특성 중에서 가장 관련성이 높거나 유용한 특성을 선택하고, 불필요한 특성을 제거하는 과정

- 차원의 저주 : 높은 차원의 데이터는 모델 학습에 시간이 오래 걸릴 뿐만 아니라, 과적합의 위험이 있음
- 계산 효율성 : 중요하지 않은 특성을 제거하면 계산 효율성이 향상됨
- 모델의 해석 : 중요한 특성만 사용하면 모델의 해석이 더 쉬워짐

### 1 Feature Selection 통계 기법

1 ANOVA (일원분산분석)

- 카테고리별 numeric data 분포 차이를 검증하는 방법
- 예를 들어, 성별에 따른 키 분포 분석 검증이 가능
- 두 개 이상의 그룹 간의 평균 차이가 통계적으로 의미 있는지 평가하는 방법
- 전제 : 정규성, 등분산성, 독립성 -> 만족하지 않으면 해당 검정을 신뢰할 수 없음 (정규성, 등분산성 만족하지 않는 경우가 많기 때문에, 비모수 검정인 크루스칼 왈리스 검정을 활용하기도 함)

2 Chi-squared Test (카이 제곱 검정)

- 두 범주형(카테고리) 변수에 대한 분석 방법
- 예를 들어, 성별에 따른 선호 영화 장르 비교 문제
- 카이 제곱 검정은 3가지 종류가 있으며, 데이터 수집 방법과 가설에 따라 나뉨.
- 적합도 검정
  - 변수 1개인 경우
  - 기존에 알려준 기준이 존재하는 검정
  - 예시 (상자 안에 공 3개가 같은 비율로 알려져 있음. 공 100개를 뽑았을 때, 각 색의 비율이 구해짐 -> 기존에 알려진 공 비율 분포를 따르는지 검정)
- 독립성 검정
  - 변수 2개인 경우
  - 범주형 두 변수가 서로 연관되어 있는지 여부를 결정
  - 예시) 성별과 흡연 여부 관계를 알고 싶어서 200명을 추출하여 조사한 경우
- 동질성 검정
  - 변수 2개인 경우
  - 범주형 두 변수의 관계를 알기 위한 검정은 아님. 각 그룹들이 동질한 지 알고 싶은 검정
  - 예시) 남자와 여자 흡연율 차이가 있는지 흡연율을 조사한 후, 두 그룹의 흡연율이 같은지 여부를 검정

### 2 Feature Selection 데이터 기법

1 Pearson Correlation Coefficient (피어슨 상관계수)

- 두 변수 간의 선형 관계의 강도와 방향을 나타내느 값
- 상관계수는 mW1과 1사이의 값을 갖음
  - 1 : 완벽하게 양의 선형 관계
  - 0 : 선형 관계 전혀 없음
  - -1 : 완벽하게 음의 선형 관계
- 예시) 아이스크림 판매량과 기온. 기온이 높아질 수록 아이스크림 판매량도 증가한다면 양의 상관관계가 있다고 볼 수 있음
- 주의점
  - 상관계수가 높다고 해서 두 변수 간에 인과 관계가 있다고 결론짓기는 어려움

2 Variance Inflation Factor (VIF)

- 회귀 분석에서 독립 변수들 간의 다중공선성을 평가하는데 사용되는 값
- 다중공선성: 두 개 이상의 독립변수가 서로 밀접하게 관련되어 있는 상황)
- VIF는 해당 변수가 다른 변수와 얼마나 관련되어 있는지를 수치로 표현
  - VIF = 1 : 변수들 간에 전혀 다중공선성이 없음
  - VIF > 10 : 보통 다중공선성이 있다고 판단하고 주의가 필요함
- 예시) 집의 면적과 방의 수. 큰 집은 방의 수도 많을 가능성이 높기 때문에, 두 변수 사이에 다중 공선성이 있을 수 있음
- 해결 방법
  - VIF 값이 높은 변수들 중 하나나 더 많은 변수를 제거
  - 변수를 결합하여 새로운 변수를 만듬

### 3 Feature Selection ML 분석 기법

- 기계 학습에서 사용되는 여러 알고리즘이 자체적으로 feature importance를 평가하고 선택할 수 있는 능력을 가지고 있음
- 데이터에 내재된 복잡한 관계를 바탕으로 중요한 특성들만을 선택할 수 있음

### 4 Feature Reduction

특성 일부를 제거하거나, 여러 특성을 결합하여 새로운 특성을 생성하는 과정

1 Principal Component Analysis (PCA)

- 선형 차원 축소 방법
- 데이터의 분산이 최대가 되는 방향으로 데이터를 투영PHProjectionp.m.하여, 원본 데이터의 차원을 축소하는 기법
- 원복 데이터의 정보 손실을 최소화하면서 차원을 축소

2 Linear Discriminant Analysis (LDA)

- 분류 문제에 적합
- 클래스 간 분산은 최대로, 클래스 내 분산은 최소로 만드는 방향으로 데이터를 투영하는 기법
- 각 클래스를 잘 구분할 수 있는 특성을 생성하는 것이 목표

3 Auto Encoder

- 딥러닝 기반의 비지도 학습 모델
- 입력 데이터를 압축된 표현으로 인코딩한 후 다시 디코딩하여 원래 입력 데이터를 재구성
- 중간의 압축된 표현은 원본 데이터의 차원보다 훨씬 작을 수 있으며, 이 표현을 사용하여 특성을 축소 할 수 있음
- 장점 : 앞의 방법과 달리 비선형 관계를 포착할 수 있으며, 대용량 데이터에서 효과적

## 3 Data Augmentation

정의

- 기존 데이터셋을 변형하여 추가적인 학습 데이터를 생성하는 기법
- 데이터에 약간의 노이즈를 추가하거나, 데이터를 회전, 확대/축소, 반전 등의 방법으로 원본 데이터와는 조금 다른 새로운 데이터를 만드는 것을 포함함

필요성

- 데이터 부족 문제 해결 : 딥러닝과 같은 복잡한 모델을 학습시킬 때 충분한 양의 학습 데이터가 필요. 추가적인 데이터 수집하지 않고도 가상의 학습 데이터를 확장할 수 있음
- 오버피팅 방지 : 다양한 변형을 가진 데이터로 학습을 시키면 모델이 더 일반적인 특징을 학습하게 됨
- 데이터 다양성 증가 : 실세계의 데이터는 항상 예측할 수 없는 다양성을 가지고 있음. Data Augmentation을 통해 이러한 다양성을 모의로 재현할 수 있음

## 4 Data Scaling & Encoding

정의

- Data Scaling : 다양한 Feature의 값 범위를 표준화하거나 정규화하는 과정. 알고리즘의 성능과 학습 속도에 큰 영향을 미칠 수
  있음
- Data Encoding : Category형 변수를 숫자형 변수로 변환하는 과정. 대부분의 머신러닝 알고리즘(컴퓨터 머신 등)은 숫자형 데이터만 처리할 수 있으므로 Encoding은 필수적인 전처리 단계.

필요성

- 성능 최적화 : 올바른 scaling과 encoding은 모델의 성능을 향상시킬 수 있음. 잘못된 scale 또는 encoding은 알고리즘이 학습을
  제대로 하지 못하게 만들 수 있음
- 학습 속도 향상 : Scaling된 데이터는 경사 하강법과 같은 최적화 알고리즘의 수렴을 빠르게 만듬
- 데이터 이해력 강화: 적절한 encoding을 통해 데이터 구조와 관계를 더 잘 이해할 수 있음

방법론

- Data Scaling

  - MinmWMax Scaling
    - 데이터를 0과 1사이의 값으로 변환하여 모든 feature의 scale을 동일하게 변환
  - Standard Scaling (ZmWScore Normalization)
    - 데이터의 평균을 0, 표준편차를 1로 변환
  - Robust Scaling
    - 중앙갑솨 IQRPHInterquartile Rangep.m.을 사용하여 이상치의 영향을 최소화
    - 장점 : 이상치의 영향을 크게 받지 않음
    - 단점 : 데이터 분포를 왜곡할 수 있음

- Data Encoding
  - Label Encoding
    - 카테고리 값을 순차적인 정수로 변환
    - 장점 : 간단하게 카테고리형 변수를 숫자로 변환할 수 있음
    - 단점 : 알고리즘이 숫자 간의 관계를 잘못 해석할 수 있음
  - One-Hot Encoding
    - 각 카테고리 값에 대해 새로운 binary feature를 생성
    - 장점 : 숫자 간의 관계를 고려하지 않아도 됨
    - 단점 : feature 수가 크게 증가할 수 있음
  - Ordinal Encoding
    - 순서가 있는 카테고리 값을 순차적인 정수로 변환
    - 장점 : 순서 정보를 유지하면서 카테고리형 변수를 숫자로 변환할 수 있음
    - 단점 : 순서가 없는 변수에 사용하면 오류를 유발할 수 있음
